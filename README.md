# ğŸ­ AI ê¸°ë°˜ ìƒì‚°ë¼ì¸ ìµœì í™” ì‹œë®¬ë ˆì´ì…˜

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

### ğŸ¯ í”„ë¡œì íŠ¸ ëª©ì 
ì´ í”„ë¡œì íŠ¸ëŠ” **ì¸ê³µì§€ëŠ¥(AI)ì„ í™œìš©í•œ ìµœì í™” ê¸°ë²•ì„ ìƒì‚°ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜ì— ì ìš©**í•˜ì—¬, ê¸°ì¡´ì˜ ê²½í—˜ì  ë˜ëŠ” ë¬´ì‘ìœ„ì  ì˜ì‚¬ê²°ì • ë°©ì‹ ëŒ€ë¹„ AIì˜ íš¨ê³¼ë¥¼ ì…ì¦í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.

### ğŸ¤– í•µì‹¬ ì•„ì´ë””ì–´
- **ë¬¸ì œ**: ìƒì‚°ë¼ì¸ì—ì„œ ê° ê³µì •(ê°€ê³µâ†’ì¡°ë¦½â†’ê²€ì‚¬)ì— ëª‡ ëŒ€ì˜ ê¸°ê³„ë¥¼ ë°°ì¹˜í•´ì•¼ ìµœì ì˜ ì„±ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆì„ê¹Œ?
- **AIì˜ ì—­í• **: ì‹¤ì‹œê°„ìœ¼ë¡œ ìƒì‚° ìƒí™©ì„ ë¶„ì„í•˜ê³ , ì²˜ë¦¬ëŸ‰ì„ ìµœëŒ€í™”í•˜ë©´ì„œ ë¹„ìš©ê³¼ ëŒ€ê¸°ì‹œê°„ì„ ìµœì†Œí™”í•˜ëŠ” ìµœì ì˜ ê¸°ê³„ ë°°ì¹˜ ì „ëµì„ í•™ìŠµ
- **ê²€ì¦ ë°©ë²•**: AI ë°©ì‹ vs ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²•ì˜ ì„±ëŠ¥ì„ ìˆ˜ì¹˜ì ìœ¼ë¡œ ë¹„êµ

### ğŸ“Š ì£¼ìš” ì„±ê³¼ (ì‹¤ì œ ì‹¤í–‰ ê²°ê³¼)
- **ì²˜ë¦¬ëŸ‰**: AIê°€ ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²• ëŒ€ë¹„ **13.0% í–¥ìƒ** (ì‹œê°„ë‹¹ ìƒì‚°ëŸ‰: AI 100.9ê°œ vs ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²• 89.3ê°œ)
- **ìš´ì˜ë¹„ìš©**: AIê°€ ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²• ëŒ€ë¹„ **52.0% ì ˆê°** (AI $3,760 vs ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²• $7,828)
- **ëŒ€ê¸°ì‹œê°„**: AIê°€ ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²• ëŒ€ë¹„ **92.3% ë‹¨ì¶•** (AI 0.16ë¶„ vs ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²• 2.09ë¶„)
- **ì „ì²´ ë³´ìƒ**: AIê°€ ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²• ëŒ€ë¹„ **101.8% í–¥ìƒ** (AI 123.7 vs ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²• 61.3)

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ

### í•µì‹¬ ê¸°ìˆ 
- **ê°•í™”í•™ìŠµ**: PPO (Proximal Policy Optimization) ì•Œê³ ë¦¬ì¦˜
- **ì‹œë®¬ë ˆì´ì…˜**: SimPy (ì´ì‚°ì‚¬ê±´ ì‹œë®¬ë ˆì´ì…˜ ë¼ì´ë¸ŒëŸ¬ë¦¬)
- **AI í”„ë ˆì„ì›Œí¬**: Stable-Baselines3
- **ë°ì´í„° ë¶„ì„**: Pandas, NumPy, Matplotlib, Seaborn

### ê°œë°œ í™˜ê²½
- **ì–¸ì–´**: Python 3.8+
- **ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬**: `gymnasium`, `stable-baselines3`, `simpy`, `matplotlib`, `seaborn`

## ğŸ“ íŒŒì¼ êµ¬ì¡° ë° ì—­í• 

### ğŸ”§ í•µì‹¬ êµ¬ì„± íŒŒì¼

#### 1. `config.py` - ì‹œë®¬ë ˆì´ì…˜ ì„¤ì • ê´€ë¦¬ì
```python
# ì£¼ìš” ì„¤ì •ê°’ ì˜ˆì‹œ
MAX_MACHINES_PER_STATION = 50  # ê° ìŠ¤í…Œì´ì…˜ë³„ ìµœëŒ€ ê¸°ê³„ ìˆ˜
SIMULATION_TIME = 60  # ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„ (ë¶„)
STATION_CONFIG = {
    'machining': {'capacity': 1, 'name': 'ê°€ê³µ ìŠ¤í…Œì´ì…˜'},
    'assembly': {'capacity': 1, 'name': 'ì¡°ë¦½ ìŠ¤í…Œì´ì…˜'},  
    'inspection': {'capacity': 1, 'name': 'ê²€ì‚¬ ìŠ¤í…Œì´ì…˜'}
}
```

**ì—­í• **: 
- ì „ì²´ ì‹œìŠ¤í…œì˜ ì„¤ì •ê°’ì„ ì¤‘ì•™ì§‘ì¤‘ì‹ìœ¼ë¡œ ê´€ë¦¬
- ìŠ¤í…Œì´ì…˜ë³„ ê¸°ê³„ ìˆ˜, ì‘ì—… ì‹œê°„, ë¶€í’ˆ íˆ¬ì… ê°„ê²© ë“±ì„ ì„¤ì •
- ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤(ë³‘ëª© ë°œìƒ, ê³ ìˆ˜ìš” ë“±) í”„ë¦¬ì…‹ ì œê³µ
- ì‹¤í—˜ ì¡°ê±´ ë³€ê²½ ì‹œ ì´ íŒŒì¼ë§Œ ìˆ˜ì •í•˜ë©´ ì „ì²´ ì‹œìŠ¤í…œì— ìë™ ë°˜ì˜

#### 2. `rl_environment.py` - ê°•í™”í•™ìŠµ í™˜ê²½ êµ¬í˜„
```python
class ProductionLineEnv(gym.Env):
    def __init__(self):
        # í–‰ë™ê³µê°„: ê° ìŠ¤í…Œì´ì…˜ì— ë°°ì¹˜í•  ê¸°ê³„ ìˆ˜ (1~50ëŒ€)
        self.action_space = spaces.MultiDiscrete([MAX_MACHINES_PER_STATION] * 3)
        
        # ìƒíƒœê³µê°„: [ì²˜ë¦¬ëŸ‰, í‰ê· ëŒ€ê¸°ì‹œê°„, ê° ìŠ¤í…Œì´ì…˜ ê°€ë™ë¥ ]
        self.observation_space = spaces.Box(low=np.array([0, 0, 0, 0, 0]), 
                                          high=np.array([np.inf, np.inf, 100, 100, 100]))
```

**í•µì‹¬ ë™ì‘ ë¡œì§**:
1. **ì´ˆê¸°í™”**: ìƒì‚°ë¼ì¸ í™˜ê²½ì„ ì„¤ì •í•˜ê³  ìƒíƒœê³µê°„/í–‰ë™ê³µê°„ ì •ì˜
2. **step() í•¨ìˆ˜**: AIê°€ ì„ íƒí•œ ê¸°ê³„ ë°°ì¹˜ë¥¼ ë°›ì•„ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
3. **ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰**: SimPyë¥¼ ì‚¬ìš©í•´ ì‹¤ì œ ìƒì‚° ê³¼ì •ì„ ëª¨ë¸ë§
4. **ë³´ìƒ ê³„ì‚°**: ì²˜ë¦¬ëŸ‰â†‘, ë¹„ìš©â†“, ëŒ€ê¸°ì‹œê°„â†“ë¥¼ ì¢…í•©í•œ ì ìˆ˜ ì‚°ì¶œ
5. **ìƒíƒœ ë°˜í™˜**: ë‹¤ìŒ ì˜ì‚¬ê²°ì •ì„ ìœ„í•œ í˜„ì¬ ìƒì‚°ë¼ì¸ ìƒíƒœ ì œê³µ

#### 3. `simple_agent_v1.py` - AI ì—ì´ì „íŠ¸ í•™ìŠµ ì‹œìŠ¤í…œ
```python
class SimpleProductionAgent:
    def train(self, total_timesteps):
        self.model = PPO("MlpPolicy", self.env, 
                        learning_rate=0.0003,
                        n_steps=1024, 
                        batch_size=64)
        self.model.learn(total_timesteps=total_timesteps)
```

**í•µì‹¬ ë™ì‘ ë¡œì§**:
1. **ëª¨ë¸ ì´ˆê¸°í™”**: PPO ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì •ì±… ë„¤íŠ¸ì›Œí¬ ìƒì„±
2. **ê²½í—˜ ìˆ˜ì§‘**: í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° (ìƒíƒœ, í–‰ë™, ë³´ìƒ) ë°ì´í„° ì¶•ì 
3. **ì •ì±… ì—…ë°ì´íŠ¸**: ìˆ˜ì§‘ëœ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ì‹ ê²½ë§ ê°€ì¤‘ì¹˜ ìµœì í™”
4. **ì„±ëŠ¥ í‰ê°€**: í•™ìŠµëœ ëª¨ë¸ê³¼ ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²• ì„ íƒ ë°©ì‹ì˜ ì„±ëŠ¥ ë¹„êµ
5. **ëª¨ë¸ ì €ì¥**: í•™ìŠµ ì™„ë£Œëœ AI ëª¨ë¸ì„ íŒŒì¼ë¡œ ì €ì¥

**PPO ì•Œê³ ë¦¬ì¦˜ ì„ íƒ ì´ìœ **:
- ì•ˆì •ì ì¸ í•™ìŠµ ì„±ëŠ¥
- ì—°ì†ì /ì´ì‚°ì  í–‰ë™ê³µê°„ ëª¨ë‘ ì§€ì›
- ì‚°ì—… í˜„ì¥ì—ì„œ ê²€ì¦ëœ ì‹ ë¢°ì„±

#### 4. `training_analysis_v1.py` - ì„±ëŠ¥ ë¶„ì„ ë° ì‹œê°í™” ë„êµ¬
```python
def run_performance_analysis(self, num_episodes=100):
    # AI vs Monte Carlo method ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘
    for agent_type in ['AI', 'Monte Carlo method']:
        for i in range(num_episodes):
            # ê° ì—í”¼ì†Œë“œë³„ ì„±ëŠ¥ ì§€í‘œ ê¸°ë¡
            agent_data.append({
                'Agent': agent_type,
                'Reward': reward,
                'Throughput': info['throughput'],
                'Cost': info['total_cost'],
                'WaitTime': avg_wait_time
            })
```

**í•µì‹¬ ë™ì‘ ë¡œì§**:
1. **ë°ì´í„° ìˆ˜ì§‘**: AIì™€ ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²•ì„ ê°ê° 100íšŒì”© ì‹¤í–‰í•˜ì—¬ ì„±ëŠ¥ ë°ì´í„° ì¶•ì 
2. **í†µê³„ ë¶„ì„**: í‰ê· , ì¤‘ê°„ê°’, í‘œì¤€í¸ì°¨ ë“± ê¸°ìˆ í†µê³„ëŸ‰ ê³„ì‚°
3. **ë¹„êµ ë¶„ì„**: AI ëŒ€ë¹„ ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²•ì˜ ì„±ëŠ¥ ê°œì„ ë¥ ì„ ë°±ë¶„ìœ¨ë¡œ ì‚°ì¶œ
4. **ì‹œê°í™”**: 4ê°œ ì°¨íŠ¸(ì²˜ë¦¬ëŸ‰, ë¹„ìš©, ëŒ€ê¸°ì‹œê°„, ì „ëµë¶„í¬)ë¡œ ê²°ê³¼ í‘œì‹œ
5. **ë³´ê³ ì„œ ìƒì„±**: í„°ë¯¸ë„ê³¼ ê·¸ë˜í”„ë¥¼ í†µí•œ ì¢…í•© ì„±ê³¼ ë¦¬í¬íŠ¸ ì œê³µ

## ğŸ”„ ì „ì²´ ì‹œìŠ¤í…œ ë™ì‘ íë¦„

### 1ë‹¨ê³„: í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
```
config.py â†’ ì‹œë®¬ë ˆì´ì…˜ íŒŒë¼ë¯¸í„° ì„¤ì •
rl_environment.py â†’ ê°•í™”í•™ìŠµ í™˜ê²½ êµ¬ì„±
```

### 2ë‹¨ê³„: AI í•™ìŠµ ê³¼ì •
```
1. ëœë¤ ê¸°ê³„ ë°°ì¹˜ë¡œ ì‹œì‘
2. ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ â†’ ê²°ê³¼ ê´€ì°°
3. ë³´ìƒì„ ë°”íƒ•ìœ¼ë¡œ ì •ì±… ê°œì„ 
4. 500,000íšŒ ë°˜ë³µ í•™ìŠµ
```

### 3ë‹¨ê³„: ì„±ëŠ¥ ê²€ì¦
```
í•™ìŠµëœ AI vs ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²• ì„ íƒ ë°©ì‹
â†’ 200íšŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
â†’ í†µê³„ì  ìœ ì˜ì„± ê²€ì¦
â†’ ê²°ê³¼ ì‹œê°í™”
```

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1. ì˜ì¡´ì„± ì„¤ì¹˜
```bash
pip install gymnasium stable-baselines3 simpy matplotlib seaborn pandas numpy
```

### 2. AI í•™ìŠµ ì‹¤í–‰
```bash
python simple_agent_v1.py
```
- í•™ìŠµ ì‹œê°„: ì•½ 30-120ë¶„ (ì»´í“¨í„° ì„±ëŠ¥ì— ë”°ë¼)
- ê²°ê³¼: `my_production_ai.zip` ëª¨ë¸ íŒŒì¼ ìƒì„±

### 3. ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰
```bash
python training_analysis_v1.py
```

### 4. ì„¤ì • ë³€ê²½ (ì„ íƒì‚¬í•­)
```python
# config.pyì—ì„œ ë‹¤ì–‘í•œ ì‹¤í—˜ ì¡°ê±´ ì„¤ì • ê°€ëŠ¥
MAX_MACHINES_PER_STATION = 30  # ê¸°ê³„ ìˆ˜ ì œí•œ ë³€ê²½
apply_scenario('bottleneck_assembly')  # ë³‘ëª© ì‹œë‚˜ë¦¬ì˜¤ ì ìš©
```

## ğŸ“ˆ ì£¼ìš” ì„±ê³¼ ì§€í‘œ í•´ì„
![ìŠ¤í¬ë¦°ìƒ· 2025-06-08 222249](https://github.com/user-attachments/assets/b6e1b7f4-fa34-4210-8f43-7ef81d9a480f)

### ğŸ¯ ì²˜ë¦¬ëŸ‰ (Throughput)
- **ì˜ë¯¸**: ì‹œê°„ë‹¹ ì™„ì„±í’ˆ ìƒì‚° ê°œìˆ˜
- **AI ì„±ê³¼**: ì‹œê°„ë‹¹ ìƒì‚°ëŸ‰ 100.9ê°œ (ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²•: 89.3ê°œ/ì‹œê°„)
- **ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸**: 13.0% ìƒì‚°ì„± í–¥ìƒ â†’ ë§¤ì¶œ ì§ê²°

### ğŸ’° ìš´ì˜ë¹„ìš© (Cost)
- **ì˜ë¯¸**: ê¸°ê³„ ìš´ì˜ì— í•„ìš”í•œ ì´ ë¹„ìš©
- **AI ì„±ê³¼**: AIì˜ í‰ê·  ìš´ì˜ ë¹„ìš©ì€ $3,760ìœ¼ë¡œ, ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²•ì˜ í‰ê·  ë¹„ìš©($7,828) ëŒ€ë¹„ 52.0% ë‚®ì•˜ìŠµë‹ˆë‹¤. íŠ¹íˆ AIëŠ” ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²•ì´ ìµœëŒ€ ìƒì‚°ëŸ‰ì„ ê¸°ë¡í–ˆì„ ë•Œì˜ ìµœì € ë¹„ìš©ë³´ë‹¤ë„ 25.1% ë” ë‚®ì€ ë¹„ìš© íš¨ìœ¨ì„±ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- **ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸**: 52.0% ë¹„ìš© ì ˆê° â†’ ìˆ˜ìµì„± ê°œì„ 

### â±ï¸ ëŒ€ê¸°ì‹œê°„ (Wait Time)
- **ì˜ë¯¸**: ë¶€í’ˆì´ ê° ê³µì •ì—ì„œ ëŒ€ê¸°í•˜ëŠ” í‰ê·  ì‹œê°„
- **AI ì„±ê³¼**: í‰ê·  0.16ë¶„ (ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²•: 2.09ë¶„)
- **ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸**: 92.3% ëŒ€ê¸°ì‹œê°„ ë‹¨ì¶• â†’ ê³ ê° ë§Œì¡±ë„ í–¥ìƒ

## ğŸ“ í•™ìŠµ ê°€ì¹˜

### ê¸°ìˆ ì  í•™ìŠµ í¬ì¸íŠ¸
1. **ê°•í™”í•™ìŠµ ì‹¤ì „ ì ìš©**: ì´ë¡ ì„ ì‹¤ì œ ë¬¸ì œì— ì ìš©í•˜ëŠ” ê²½í—˜
2. **ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸ë§**: ë³µì¡í•œ ì‹œìŠ¤í…œì„ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” ëŠ¥ë ¥
3. **ì„±ëŠ¥ ìµœì í™”**: ë‹¤ëª©ì  ìµœì í™” ë¬¸ì œ í•´ê²° ê²½í—˜
4. **ë°ì´í„° ë¶„ì„**: ì‹¤í—˜ ê²°ê³¼ë¥¼ ê³¼í•™ì ìœ¼ë¡œ ë¶„ì„í•˜ëŠ” ë°©ë²•ë¡ 

## ğŸ”® í–¥í›„ ê°œì„  ë°©í–¥

### ê°œì„  ì‚¬í•­
- [ ] ë” ë³µì¡í•œ ìƒì‚°ë¼ì¸ (4ê°œ ì´ìƒ ìŠ¤í…Œì´ì…˜) ì§€ì›
- [ ] ì‹¤ì‹œê°„ ìˆ˜ìš” ë³€ë™ ë°˜ì˜
- [ ] ê¸°ê³„ ê³ ì¥/ìœ ì§€ë³´ìˆ˜ ìƒí™© ëª¨ë¸ë§
- [ ] ë‹¤ì–‘í•œ ì œí’ˆ íƒ€ì… ë™ì‹œ ìƒì‚°

## ğŸ“ ì—°ë½ì²˜ ë° ì¶”ê°€ ì •ë³´

ì´ í”„ë¡œì íŠ¸ì— ëŒ€í•œ ì¶”ê°€ ë¬¸ì˜ëŠ” ì–¸ì œë“  í™˜ì˜í•©ë‹ˆë‹¤!

---
*ì´ í”„ë¡œì íŠ¸ëŠ” AI ê¸°ìˆ ì˜ ì‹¤ìš©ì  ê°€ì¹˜ë¥¼ ì…ì¦í•˜ê³ , ì œì¡°ì—… í˜ì‹ ì— ê¸°ì—¬í•˜ê³ ì í•˜ëŠ” ëª©ì ìœ¼ë¡œ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.*



# ğŸ­ AI-Powered Production Line Optimization Simulation

## ğŸ“‹ Project Overview

### ğŸ¯ Project Purpose
This project aims to **apply AI-powered optimization techniques to production line simulation** and demonstrate the effectiveness of AI compared to traditional empirical or Monte Carlo method decision-making approaches.

### ğŸ¤– Core Concept
- **Problem**: How many machines should be deployed at each production stage (Machiningâ†’Assemblyâ†’Inspection) to achieve optimal performance?
- **AI's Role**: Analyze production situations in real-time and learn optimal machine allocation strategies that maximize throughput while minimizing costs and wait times
- **Validation Method**: Quantitative performance comparison between AI approach vs. Monte Carlo method

### ğŸ“Š Key Performance Results (Actual Execution)
- **Throughput**: AI achieved **13.0% improvement** over Monte Carlo method (AI 100.9 units/hour vs Monte Carlo method 89.3 units/hour)
- **Operating Cost**: AI achieved **52.0% cost reduction** compared to Monte Carlo method (AI $3,760 vs Monte Carlo method $7,828)
- **Wait Time**: AI achieved **92.3% wait time reduction** compared to Monte Carlo method (AI 0.16 min vs Monte Carlo method 2.09 min)
- **Overall Reward**: AI achieved **101.8% improvement** over Monte Carlo method (AI 123.7 vs Monte Carlo method 61.3)

## ğŸ› ï¸ Technology Stack

### Core Technologies
- **Reinforcement Learning**: PPO (Proximal Policy Optimization) Algorithm
- **Simulation**: SimPy (Discrete Event Simulation Library)
- **AI Framework**: Stable-Baselines3
- **Data Analysis**: Pandas, NumPy, Matplotlib, Seaborn

### Development Environment
- **Language**: Python 3.8+
- **Key Libraries**: `gymnasium`, `stable-baselines3`, `simpy`, `matplotlib`, `seaborn`

## ğŸ“ File Structure and Roles

### ğŸ”§ Core Components

#### 1. `config.py` - Simulation Configuration Manager
```python
# Key configuration examples
MAX_MACHINES_PER_STATION = 50  # Maximum machines per station
SIMULATION_TIME = 60  # Simulation time (minutes)
STATION_CONFIG = {
    'machining': {'capacity': 1, 'name': 'Machining Station'},
    'assembly': {'capacity': 1, 'name': 'Assembly Station'},  
    'inspection': {'capacity': 1, 'name': 'Inspection Station'}
}
```

**Role**: 
- Centralized management of system-wide configuration values
- Configuration of machine counts per station, work times, part arrival intervals, etc.
- Provides various scenario presets (bottlenecks, high demand, etc.)
- Changes to this file automatically reflect across the entire system

#### 2. `rl_environment.py` - Reinforcement Learning Environment Implementation
```python
class ProductionLineEnv(gym.Env):
    def __init__(self):
        # Action space: Number of machines to deploy at each station (1~50 units)
        self.action_space = spaces.MultiDiscrete([MAX_MACHINES_PER_STATION] * 3)
        
        # State space: [throughput, avg_wait_time, utilization_rate_per_station]
        self.observation_space = spaces.Box(low=np.array([0, 0, 0, 0, 0]), 
                                          high=np.array([np.inf, np.inf, 100, 100, 100]))
```

**Core Operation Logic**:
1. **Initialization**: Set up production line environment and define state/action spaces
2. **step() Function**: Receive AI's machine allocation choice and execute simulation
3. **Simulation Execution**: Model actual production processes using SimPy
4. **Reward Calculation**: Calculate comprehensive score considering throughputâ†‘, costâ†“, wait timeâ†“
5. **State Return**: Provide current production line status for next decision-making

#### 3. `simple_agent_v1.py` - AI Agent Training System
```python
class SimpleProductionAgent:
    def train(self, total_timesteps):
        self.model = PPO("MlpPolicy", self.env, 
                        learning_rate=0.0003,
                        n_steps=1024, 
                        batch_size=64)
        self.model.learn(total_timesteps=total_timesteps)
```

**Core Operation Logic**:
1. **Model Initialization**: Create policy network using PPO algorithm
2. **Experience Collection**: Accumulate (state, action, reward) data through environment interaction
3. **Policy Update**: Optimize neural network weights based on collected experience
4. **Performance Evaluation**: Compare performance between trained model and Monte Carlo method selection
5. **Model Saving**: Save trained AI model to file

**PPO Algorithm Selection Rationale**:
- Stable learning performance
- Support for both continuous and discrete action spaces
- Proven reliability in industrial applications

#### 4. `training_analysis_v1.py` - Performance Analysis and Visualization Tool
```python
def run_performance_analysis(self, num_episodes=100):
    # Collect AI vs Monte Carlo method agent performance data
    for agent_type in ['AI', 'Monte Carlo method']:
        for i in range(num_episodes):
            # Record performance metrics for each episode
            agent_data.append({
                'Agent': agent_type,
                'Reward': reward,
                'Throughput': info['throughput'],
                'Cost': info['total_cost'],
                'WaitTime': avg_wait_time
            })
```

**Core Operation Logic**:
1. **Data Collection**: Execute AI and Monte Carlo method 100 times each to accumulate performance data
2. **Statistical Analysis**: Calculate descriptive statistics (mean, median, standard deviation)
3. **Comparative Analysis**: Calculate AI performance improvement rates as percentages
4. **Visualization**: Display results through 4 charts (throughput, cost, wait time, strategy distribution)
5. **Report Generation**: Provide comprehensive performance reports via terminal and graphs

## ğŸ”„ Overall System Workflow

### Stage 1: Environment Setup and Initialization
```
config.py â†’ Set simulation parameters
rl_environment.py â†’ Configure reinforcement learning environment
```

### Stage 2: AI Learning Process
```
1. Start with random machine allocation
2. Execute simulation â†’ Observe results
3. Improve policy based on rewards
4. Repeat learning for 500,000 iterations
```

### Stage 3: Performance Validation
```
Trained AI vs Monte Carlo method selection approach
â†’ Execute 200 test runs
â†’ Statistical significance verification
â†’ Result visualization
```

## ğŸš€ How to Run

### 1. Install Dependencies
```bash
pip install gymnasium stable-baselines3 simpy matplotlib seaborn pandas numpy
```

### 2. Execute AI Training
```bash
python simple_agent_v1.py
```
- Training time: Approximately 30-120 minutes (depending on computer performance)
- Result: Generates `my_production_ai.zip` model file

### 3. Execute Performance Analysis
```bash
python training_analysis_v1.py
```

### 4. Configuration Changes (Optional)
```python
# Various experimental conditions can be set in config.py
MAX_MACHINES_PER_STATION = 30  # Change machine count limit
apply_scenario('bottleneck_assembly')  # Apply bottleneck scenario
```

## ğŸ“ˆ Key Performance Metrics Interpretation
![ìŠ¤í¬ë¦°ìƒ· 2025-06-08 222249](https://github.com/user-attachments/assets/dcc27d68-f4e9-4faa-8fbc-540bfe4969ff)

### ğŸ¯ Throughput
- **Meaning**: Number of finished products per hour
- **AI Performance**: 100.9 units/hour (Monte Carlo method: 89.3 units/hour)
- **Business Impact**: 13.0% productivity improvement â†’ Direct revenue impact

### ğŸ’° Operating Cost
- **Meaning**: Total cost required for machine operation
- **AI Performance**: AI's average operating cost was $3,760, which was 52.0% lower than the Monte Carlo method method's average cost ($7,828). Notably, AI demonstrated 25.1% better cost efficiency than the Monte Carlo method method's 'lowest cost' when it achieved 'maximum production'.
- **Business Impact**: 52.0% cost reduction â†’ Profitability improvement

### â±ï¸ Wait Time
- **Meaning**: Average time parts wait at each process
- **AI Performance**: Average 0.16 minutes (Monte Carlo method: 2.09 minutes)
- **Business Impact**: 92.3% wait time reduction â†’ Customer satisfaction improvement

## ğŸ“ Learning Value

### Technical Learning Points
1. **Practical Reinforcement Learning Application**: Experience applying theory to real problems
2. **Simulation Modeling**: Ability to mathematically model complex systems
3. **Performance Optimization**: Experience solving multi-objective optimization problems
4. **Data Analysis**: Methodology for scientifically analyzing experimental results

## ğŸ”® Future Improvement Directions

### Improvement Items
- [ ] Support for more complex production lines (4+ stations)
- [ ] Real-time demand fluctuation reflection
- [ ] Machine failure/maintenance situation modeling
- [ ] Simultaneous production of various product types

## ğŸ“ Contact and Additional Information

Inquiries about this project are always welcome!

---
*This project was developed to demonstrate the practical value of AI technology and contribute to manufacturing innovation.*
