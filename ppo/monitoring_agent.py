# simple_agent_with_plot.py

from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env
import numpy as np
# stable_baselines3ì—ì„œ ì œê³µí•˜ëŠ” Monitorë¥¼ ì„í¬íŠ¸í•˜ì—¬ ë³´ìƒì„ ì‰½ê²Œ ê¸°ë¡í•©ë‹ˆë‹¤.
from stable_baselines3.common.monitor import Monitor
# ê·¸ë˜í”„ ìƒì„±ì„ ìœ„í•´ matplotlib.pyplotì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
import matplotlib.pyplot as plt

# rl_environment.pyì™€ config.py íŒŒì¼ì€ ë™ì¼í•œ ë””ë ‰í† ë¦¬ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
from rl_environment import ProductionLineEnv
from config import MAX_MACHINES_PER_STATION

class SimpleProductionAgent:
    """
    ë™ì  ìƒì‚°ë¼ì¸ í™˜ê²½ì— ë§ì¶° í•™ìŠµí•˜ëŠ” AI ì—ì´ì „íŠ¸.
    ì—í”¼ì†Œë“œë³„ ë³´ìƒì„ ê¸°ë¡í•˜ê³  í•™ìŠµ ì™„ë£Œ í›„ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ëŠ” ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.
    """

    def __init__(self):
        # Monitorë¡œ í™˜ê²½ì„ ê°ì‹¸ì£¼ë©´ ì—í”¼ì†Œë“œë³„ ë³´ìƒ, ìŠ¤í… ìˆ˜ ë“±ì´ ìë™ìœ¼ë¡œ ê¸°ë¡ë©ë‹ˆë‹¤.
        self.env = Monitor(ProductionLineEnv())
        self.model = None
        self.is_trained = False

    def train(self, total_timesteps):
        """AI ì—ì´ì „íŠ¸ í•™ìŠµ"""
        print("ğŸ” í™˜ê²½ ê²€ì¦ ì¤‘...")

        try:
            check_env(self.env)
            print("âœ… í™˜ê²½ ê²€ì¦ ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ í™˜ê²½ ì˜¤ë¥˜: {e}")
            return False

        print(f"ğŸ§  AI í•™ìŠµ ì‹œì‘ (ì´ {total_timesteps} ìŠ¤í…)")

        self.model = PPO(
            "MlpPolicy",
            self.env,
            verbose=1,
            learning_rate=0.0003,
            n_steps=1024,
            batch_size=64,
            gamma=0.99,
            ent_coef=0.01,
            clip_range=0.2,
            tensorboard_log="./ppo_production_tensorboard/"
        )

        self.model.learn(total_timesteps=total_timesteps)
        self.is_trained = True

        print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        return True

    def plot_training_rewards(self):
        """í•™ìŠµ ê³¼ì •ì—ì„œ ê¸°ë¡ëœ ì—í”¼ì†Œë“œë³„ ë³´ìƒì„ ê·¸ë˜í”„ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.is_trained:
            print("âŒ ë¨¼ì € í•™ìŠµì„ ì™„ë£Œí•´ì£¼ì„¸ìš”!")
            return

        # Monitor í™˜ê²½ì—ì„œ get_episode_rewards()ë¥¼ í†µí•´ ëª¨ë“  ì—í”¼ì†Œë“œì˜ ë³´ìƒ ê¸°ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        rewards = self.env.get_episode_rewards()

        if not rewards:
            print("âš ï¸ ë³´ìƒ ê¸°ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµ ì‹œê°„ì´ ë„ˆë¬´ ì§§ì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return

        print(f"ğŸ“Š ì´ {len(rewards)}ê°œ ì—í”¼ì†Œë“œì— ëŒ€í•œ ë³´ìƒ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

        # ì´ë™ í‰ê· ì„ ê³„ì‚°í•˜ì—¬ í•™ìŠµ ì¶”ì„¸ë¥¼ ë” ëª…í™•í•˜ê²Œ í™•ì¸í•©ë‹ˆë‹¤.
        window = 100
        if len(rewards) < window:
            window = len(rewards) // 5 if len(rewards) > 10 else 1

        moving_average = []
        if window > 0:
            moving_average = np.convolve(rewards, np.ones(window)/window, mode='valid')

        plt.figure(figsize=(12, 6))
        plt.plot(rewards, label='Episodic Reward', alpha=0.5)
        if len(moving_average) > 0:
            # ì´ë™ í‰ê·  ê·¸ë˜í”„ì˜ ì‹œì‘ì ì„ ë§ì¶”ê¸° ìœ„í•´ xì¶• ì¸ë±ìŠ¤ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.
            plt.plot(np.arange(window-1, len(rewards)), moving_average,
                     label=f'Moving Average (window={window})', color='red', linewidth=2)

        # ìš”ì²­ëŒ€ë¡œ ê·¸ë˜í”„ì˜ ì œëª©ê³¼ ë ˆì´ë¸”ì„ ì˜ë¬¸ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        plt.title('Training Progress: Reward per Episode')
        plt.xlabel('Episode')
        plt.ylabel('Reward')
        plt.legend(loc='upper left')
        plt.grid(True)

        # ìƒì„±ëœ í”Œë¡¯ì„ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        plt.savefig("training_reward_plot.png")
        print("âœ… ë³´ìƒ ê·¸ë˜í”„ê°€ 'training_reward_plot.png' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def test_agent(self, num_tests=5):
        """í•™ìŠµëœ ì—ì´ì „íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        if not self.is_trained:
            print("âŒ ë¨¼ì € í•™ìŠµì„ ì™„ë£Œí•´ì£¼ì„¸ìš”!")
            return None

        print(f"\nğŸ§ª í•™ìŠµëœ AI ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ({num_tests}íšŒ)")
        print("="*50)

        results = []
        total_rewards = []
        total_throughputs = []
        total_costs = []

        # í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” í•™ìŠµ ê¸°ë¡ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ ìƒˆë¡œìš´ í™˜ê²½ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        test_env = ProductionLineEnv()

        for test_num in range(num_tests):
            obs, _ = test_env.reset()
            action, _ = self.model.predict(obs, deterministic=True)
            machines = action + 1

            next_obs, reward, terminated, truncated, info = test_env.step(action)

            results.append({
                'test_num': test_num + 1,
                'action': machines,
                'reward': reward,
                'throughput': info['throughput'],
                'cost': info['total_cost']
            })
            total_rewards.append(reward)
            total_throughputs.append(info['throughput'])
            total_costs.append(info['total_cost'])

            print(f"í…ŒìŠ¤íŠ¸ {test_num + 1}:")
            print(f"  ê¸°ê³„ ë°°ì¹˜: ê°€ê³µ({machines[0]}), ì¡°ë¦½({machines[1]}), ê²€ì‚¬({machines[2]})")
            print(f"  ì²˜ë¦¬ëŸ‰: {info['throughput']:.1f}ê°œ/ì‹œê°„, ë¹„ìš©: ${info['total_cost']}, ë³´ìƒ: {reward:.2f}")

        print("\nğŸ“Š AI ì„±ëŠ¥ ìš”ì•½:")
        print(f"  í‰ê·  ì²˜ë¦¬ëŸ‰: {np.mean(total_throughputs):.1f} ê°œ/ì‹œê°„")
        print(f"  í‰ê·  ë¹„ìš©: ${np.mean(total_costs):.0f}")
        print(f"  í‰ê·  ë³´ìƒ: {np.mean(total_rewards):.2f}")

        return results

    def compare_with_random(self, num_trials=5):
        """ëœë¤ ì„ íƒê³¼ AI ì„±ëŠ¥ ë¹„êµ"""
        if not self.is_trained:
            print("âŒ ë¨¼ì € í•™ìŠµì„ ì™„ë£Œí•´ì£¼ì„¸ìš”!")
            return

        print(f"\nğŸ¥Š AI vs ëœë¤ ì„±ëŠ¥ ë¹„êµ ({num_trials}íšŒ í‰ê· )")
        print("="*50)

        test_env = ProductionLineEnv()
        ai_rewards, ai_throughputs = [], []
        for _ in range(num_trials):
            obs, _ = test_env.reset()
            ai_action, _ = self.model.predict(obs, deterministic=True)
            _, reward, _, _, info = test_env.step(ai_action)
            ai_rewards.append(reward)
            ai_throughputs.append(info['throughput'])

        avg_ai_reward = np.mean(ai_rewards)
        avg_ai_throughput = np.mean(ai_throughputs)

        random_rewards, random_throughputs = [], []
        for _ in range(num_trials):
            obs, _ = test_env.reset()
            random_action = test_env.action_space.sample()
            _, reward, _, _, info = test_env.step(random_action)
            random_rewards.append(reward)
            random_throughputs.append(info['throughput'])

        avg_random_reward = np.mean(random_rewards)
        avg_random_throughput = np.mean(random_throughputs)

        obs, _ = test_env.reset()
        ai_action, _ = self.model.predict(obs, deterministic=True)
        machines = ai_action + 1

        print(f"ğŸ¤– AI ì—ì´ì „íŠ¸ (í‰ê· ):")
        print(f"  - ëŒ€í‘œ ì „ëµ: ê°€ê³µ({machines[0]}), ì¡°ë¦½({machines[1]}), ê²€ì‚¬({machines[2]})")
        print(f"  - í‰ê·  ì²˜ë¦¬ëŸ‰: {avg_ai_throughput:.1f} ê°œ/ì‹œê°„")
        print(f"  - í‰ê·  ë³´ìƒ: {avg_ai_reward:.2f}")
        print()
        print(f"ğŸ² ëœë¤ ì„ íƒ (í‰ê· ):")
        print(f"  - í‰ê·  ì²˜ë¦¬ëŸ‰: {avg_random_throughput:.1f} ê°œ/ì‹œê°„")
        print(f"  - í‰ê·  ë³´ìƒ: {avg_random_reward:.2f}")
        print()

        throughput_improvement = ((avg_ai_throughput - avg_random_throughput) / (avg_random_throughput + 1e-6)) * 100
        reward_improvement = ((avg_ai_reward - avg_random_reward) / (abs(avg_random_reward) + 1e-6)) * 100

        print(f"ğŸ“ˆ AI ì„±ëŠ¥ í–¥ìƒ:")
        print(f"  ì²˜ë¦¬ëŸ‰ í–¥ìƒ: {throughput_improvement:+.1f}%")
        print(f"  ë³´ìƒ í–¥ìƒ: {reward_improvement:+.1f}%")

        if avg_ai_reward > avg_random_reward:
            print("ğŸ† AIê°€ ëœë¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤!")
        else:
            print("ğŸ¤” AIê°€ ë” í•™ìŠµì´ í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤.")

    def save_model(self, filename="production_agent"):
        """í•™ìŠµëœ ëª¨ë¸ ì €ì¥"""
        if not self.is_trained:
            print("âŒ ì €ì¥í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
            return False
        self.model.save(filename)
        print(f"ğŸ’¾ ëª¨ë¸ì´ '{filename}.zip'ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True

    def load_model(self, filename="production_agent"):
        """ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°"""
        try:
            self.model = PPO.load(filename, env=self.env)
            self.is_trained = True
            print(f"ğŸ“‚ ëª¨ë¸ '{filename}.zip'ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
            return True
        except FileNotFoundError:
            print(f"âŒ ëª¨ë¸ íŒŒì¼ '{filename}.zip'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False


# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    print("ğŸš€ ìƒì‚°ë¼ì¸ AI ì—ì´ì „íŠ¸ ì‹¤í–‰")
    print(f"ğŸ”© ì„¤ì •ëœ ìµœëŒ€ ê¸°ê³„ ìˆ˜: {MAX_MACHINES_PER_STATION}ëŒ€/ìŠ¤í…Œì´ì…˜")

    agent = SimpleProductionAgent()

    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ í•™ìŠµ ìŠ¤í… ìˆ˜ë¥¼ 20,000ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
    # ë” ë†’ì€ ì„±ëŠ¥ì„ ì›í•˜ì‹œë©´ 100,000 ì´ìƒìœ¼ë¡œ ê°’ì„ ë†’ì—¬ì„œ ì‹¤í–‰í•´ë³´ì„¸ìš”.
    print("\n1ï¸âƒ£ AI í•™ìŠµ ì‹œì‘...")
    success = agent.train(total_timesteps=200000)

    if success:
        # 2ï¸âƒ£ í•™ìŠµ ì™„ë£Œ í›„ ë³´ìƒ ê·¸ë˜í”„ ìƒì„±
        print("\n2ï¸âƒ£ í•™ìŠµ ë³´ìƒ ê·¸ë˜í”„ ìƒì„±...")
        agent.plot_training_rewards()

        # 3ï¸âƒ£ í•™ìŠµëœ AI í…ŒìŠ¤íŠ¸
        print("\n3ï¸âƒ£ í•™ìŠµëœ AI í…ŒìŠ¤íŠ¸...")
        agent.test_agent(num_tests=5)

        # 4ï¸âƒ£ ëœë¤ ì—ì´ì „íŠ¸ì™€ ì„±ëŠ¥ ë¹„êµ
        print("\n4ï¸âƒ£ ì„±ëŠ¥ ë¹„êµ...")
        agent.compare_with_random(num_trials=10)

        # 5ï¸âƒ£ í•™ìŠµëœ ëª¨ë¸ ì €ì¥
        print("\n5ï¸âƒ£ ëª¨ë¸ ì €ì¥...")
        agent.save_model("my_production_ai")