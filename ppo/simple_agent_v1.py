from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env
import numpy as np
import matplotlib.pyplot as plt
from stable_baselines3.common.callbacks import BaseCallback
import os
import signal
import sys
from tensorboard.backend.event_processing import event_accumulator

# ìˆ˜ì •ëœ ë™ì  í™˜ê²½ì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from rl_environment import ProductionLineEnv
# configì—ì„œ ìµœëŒ€ ê¸°ê³„ ëŒ€ìˆ˜ ì„¤ì •ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from config import MAX_MACHINES_PER_STATION

class CustomTensorboardCallback(BaseCallback):
    """
    í•™ìŠµ ì¤‘ ì£¼ê¸°ì ìœ¼ë¡œ ë³´ìƒ ë° ì†ì‹¤ì„ ê¸°ë¡í•˜ê³ , í•™ìŠµ ì™„ë£Œ ì‹œ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ê¸° ìœ„í•œ ì½œë°±.
    """
    def __init__(self, save_path, verbose=0):
        super(CustomTensorboardCallback, self).__init__(verbose)
        self.save_path = save_path
        self.episode_rewards = []
        self.episode_losses = []
        self.ep_rew_mean = 0
        self.train_loss = 0

    def _on_step(self) -> bool:
        # PPOëŠ” n_stepsë§ˆë‹¤ í•œ ë²ˆì”© í•™ìŠµì„ ì—…ë°ì´íŠ¸í•˜ë©°, ê·¸ ì‹œì ì— lossê°€ ê³„ì‚°ë©ë‹ˆë‹¤.
        # ë”°ë¼ì„œ lossëŠ” ë°”ë¡œ ì§ì „ ìŠ¤í…ì—ì„œ ê°±ì‹ ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” infoì—ì„œ lossë¥¼ ì§ì ‘ ê°€ì ¸ì˜¤ê¸° ì–´ë µê¸° ë•Œë¬¸ì—, on_rollout_endì—ì„œ ê°±ì‹ ëœ ê°’ì„ ì‚¬ìš©í•˜ê±°ë‚˜
        # Tensorboard ë¡œê·¸ì—ì„œ ì§ì ‘ íŒŒì‹±í•˜ëŠ” ë°©ì‹ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.
        # SimplePPO Agentì˜ verbose=1 ë¡œê·¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°’ì— ì ‘ê·¼í•˜ëŠ” ê²ƒì€ ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤.
        # ì‹¤ì œ loss ê°’ì€ model.loggerì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” TensorBoard ë¡œê·¸ íŒŒì‹±ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
        return True

    def _on_rollout_end(self) -> None:
        """
        ìƒˆë¡œìš´ ë¡¤ì•„ì›ƒì´ ëë‚  ë•Œë§ˆë‹¤ í˜¸ì¶œë©ë‹ˆë‹¤.
        """
        # Tensorboard ë¡œê·¸ì—ì„œ ìµœì‹  ë³´ìƒ ë° ì†ì‹¤ ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        try:
            ea = event_accumulator.EventAccumulator(self.logger.dir)
            ea.Reload()

            if 'rollout/ep_rew_mean' in ea.Tags()['scalars']:
                latest_rew_event = ea.Scalars('rollout/ep_rew_mean')[-1]
                self.ep_rew_mean = latest_rew_event.value
                self.episode_rewards.append((latest_rew_event.step, self.ep_rew_mean))

            if 'train/loss' in ea.Tags()['scalars']:
                latest_loss_event = ea.Scalars('train/loss')[-1]
                self.train_loss = latest_loss_event.value
                self.episode_losses.append((latest_loss_event.step, self.train_loss))

        except Exception as e:
            if self.verbose > 0:
                print(f"Error reading TensorBoard logs in callback: {e}")
            pass # ë¡œê·¸ íŒŒì¼ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ

class SimpleProductionAgent:
    """
    ë™ì  ìƒì‚°ë¼ì¸ í™˜ê²½ì— ë§ì¶° í•™ìŠµí•˜ëŠ” AI ì—ì´ì „íŠ¸.
    rl_environment.pyì˜ ë³€ê²½ì‚¬í•­(MultiDiscrete Action Space)ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ì‘ë™í•©ë‹ˆë‹¤.
    """

    def __init__(self):
        self.env = ProductionLineEnv()
        self.model = None
        self.is_trained = False
        self.tensorboard_log_dir = "./ppo_production_tensorboard/"
        self.callback = None
        self.trained_model_filename = "my_production_ai"

        # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ì„¤ì •
        signal.signal(signal.SIGINT, self._signal_handler)

    def _signal_handler(self, signum, frame):
        print("\nğŸš¨ Ctrl+C ê°ì§€! í•™ìŠµì„ ì¤‘ë‹¨í•˜ê³  ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.")
        if self.is_trained or (self.model is not None and self.model.num_timesteps > 0):
            self.save_model(self.trained_model_filename)
            print("âœ… í•™ìŠµëœ ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            if self.callback:
                self.plot_learning_curves(self.tensorboard_log_dir)
                print("âœ… í•™ìŠµ ê·¸ë˜í”„ê°€ ì¶œë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("âš ï¸ í•™ìŠµ ì¤‘ì´ ì•„ë‹ˆê±°ë‚˜ ì €ì¥í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(0) # í”„ë¡œê·¸ë¨ ì¢…ë£Œ

    def train(self, total_timesteps):
        """AI ì—ì´ì „íŠ¸ í•™ìŠµ"""
        print("ğŸ” í™˜ê²½ ê²€ì¦ ì¤‘...")

        try:
            # check_envê°€ ë™ì ìœ¼ë¡œ ì„¤ì •ëœ í™˜ê²½(MultiDiscrete í¬í•¨)ì„ ê²€ì¦í•©ë‹ˆë‹¤.
            check_env(self.env)
            print("âœ… í™˜ê²½ ê²€ì¦ ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ í™˜ê²½ ì˜¤ë¥˜: {e}")
            return False

        print(f"ğŸ§  AI í•™ìŠµ ì‹œì‘ (ì´ {total_timesteps} ìŠ¤í…)")

        # PPO ì•Œê³ ë¦¬ì¦˜ì€ 'MlpPolicy'ë¥¼ í†µí•´ MultiDiscrete í–‰ë™ ê³µê°„ì„ ì§€ì›í•©ë‹ˆë‹¤.
        # GPU ì‚¬ìš©ì„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì • (ìë™ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° GPU ì‚¬ìš©)
        self.model = PPO(
            "MlpPolicy",
            self.env,
            verbose=1,
            learning_rate=0.0003,
            n_steps=1024,      # ë” ë§ì€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ í›„ ì—…ë°ì´íŠ¸í•˜ì—¬ ì•ˆì •ì„± í–¥ìƒ
            batch_size=64,
            gamma=0.99,        # ë¯¸ë˜ ë³´ìƒì— ëŒ€í•œ í• ì¸ìœ¨
            ent_coef=0.01,     # íƒí—˜ì„ ì¥ë ¤í•˜ê¸° ìœ„í•œ ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜
            clip_range=0.2,    # PPOì˜ í´ë¦¬í•‘ ë²”ìœ„
            tensorboard_log=self.tensorboard_log_dir, # í•™ìŠµ ê³¼ì •ì„ í…ì„œë³´ë“œì— ê¸°ë¡
            device="auto"      # GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° GPU ì‚¬ìš©, ì•„ë‹ˆë©´ CPU ì‚¬ìš©
        )
        
        # ì»¤ìŠ¤í…€ ì½œë°± ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.callback = CustomTensorboardCallback(save_path=self.tensorboard_log_dir)

        # í•™ìŠµ ì‹¤í–‰
        self.model.learn(total_timesteps=total_timesteps, callback=self.callback)
        self.is_trained = True

        print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        return True

    def plot_learning_curves(self, log_dir, save_filename="learning_curves.png"):
        """
        TensorBoard ë¡œê·¸ íŒŒì¼ì—ì„œ ë³´ìƒ ë° ì†ì‹¤ ë°ì´í„°ë¥¼ ì½ì–´ ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.
        """
        print("\nğŸ“ˆ í•™ìŠµ ê³¡ì„  ìƒì„± ì¤‘...")
        event_file = None
        for root, _, files in os.walk(log_dir):
            for file in files:
                if "events.out.tfevents" in file:
                    event_file = os.path.join(root, file)
                    break
            if event_file:
                break

        if not event_file:
            print(f"âŒ TensorBoard ì´ë²¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {log_dir}")
            return

        ea = event_accumulator.EventAccumulator(event_file)
        ea.Reload()

        rewards = []
        losses = []
        timesteps_reward = []
        timesteps_loss = []

        if 'rollout/ep_rew_mean' in ea.Tags()['scalars']:
            for event in ea.Scalars('rollout/ep_rew_mean'):
                timesteps_reward.append(event.step)
                rewards.append(event.value)
        else:
            print("âš ï¸ 'rollout/ep_rew_mean' ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if 'train/loss' in ea.Tags()['scalars']:
            for event in ea.Scalars('train/loss'):
                timesteps_loss.append(event.step)
                losses.append(event.value)
        else:
            print("âš ï¸ 'train/loss' ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if not rewards and not losses:
            print("âŒ ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        plt.figure(figsize=(12, 6))

        if rewards:
            plt.subplot(1, 2, 1)
            plt.plot(timesteps_reward, rewards)
            plt.title('Average Episode Reward over Timesteps')
            plt.xlabel('Timesteps')
            plt.ylabel('Average Reward')
            plt.grid(True)

        if losses:
            plt.subplot(1, 2, 2)
            plt.plot(timesteps_loss, losses)
            plt.title('Training Loss over Timesteps')
            plt.xlabel('Timesteps')
            plt.ylabel('Loss')
            plt.grid(True)

        plt.tight_layout()

        # ê·¸ë˜í”„ ì €ì¥ ê¸°ëŠ¥ ì¶”ê°€
        save_path = os.path.join(log_dir, save_filename)
        plt.savefig(save_path)
        print(f"âœ… í•™ìŠµ ê·¸ë˜í”„ê°€ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        plt.show()
        print("âœ… í•™ìŠµ ê·¸ë˜í”„ê°€ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.")


    def test_agent(self, num_tests=5):
        """í•™ìŠµëœ ì—ì´ì „íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        if not self.is_trained:
            print("âŒ ë¨¼ì € í•™ìŠµì„ ì™„ë£Œí•´ì£¼ì„¸ìš”!")
            return None

        print(f"\nğŸ§ª í•™ìŠµëœ AI ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ({num_tests}íšŒ)")
        print("="*50)

        results = []
        total_rewards = []
        total_throughputs = []
        total_costs = []

        for test_num in range(num_tests):
            obs, _ = self.env.reset()
            action, _ = self.model.predict(obs, deterministic=True)
            # action ê°’ì— 1ì„ ë”í•´ ì‹¤ì œ ê¸°ê³„ ëŒ€ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤ (0-based -> 1-based)
            machines = action + 1

            next_obs, reward, terminated, truncated, info = self.env.step(action)

            results.append({
                'test_num': test_num + 1,
                'action': machines, # ì‹¤ì œ ê¸°ê³„ ëŒ€ìˆ˜ë¡œ ì €ì¥
                'reward': reward,
                'throughput': info['throughput'],
                'cost': info['total_cost']
            })
            total_rewards.append(reward)
            total_throughputs.append(info['throughput'])
            total_costs.append(info['total_cost'])

            print(f"í…ŒìŠ¤íŠ¸ {test_num + 1}:")
            print(f"  ê¸°ê³„ ë°°ì¹˜: ê°€ê³µ({machines[0]}), ì¡°ë¦½({machines[1]}), ê²€ì‚¬({machines[2]})")
            print(f"  ì²˜ë¦¬ëŸ‰: {info['throughput']:.1f}ê°œ/ì‹œê°„, ë¹„ìš©: ${info['total_cost']}, ë³´ìƒ: {reward:.2f}")

        print("\nğŸ“Š AI ì„±ëŠ¥ ìš”ì•½:")
        print(f"  í‰ê·  ì²˜ë¦¬ëŸ‰: {np.mean(total_throughputs):.1f} ê°œ/ì‹œê°„")
        print(f"  í‰ê·  ë¹„ìš©: ${np.mean(total_costs):.0f}")
        print(f"  í‰ê·  ë³´ìƒ: {np.mean(total_rewards):.2f}")

        return results

    def compare_with_random(self, num_trials=5):
        """ëœë¤ ì„ íƒê³¼ AI ì„±ëŠ¥ ë¹„êµ"""
        if not self.is_trained:
            print("âŒ ë¨¼ì € í•™ìŠµì„ ì™„ë£Œí•´ì£¼ì„¸ìš”!")
            return

        print(f"\nğŸ¥Š AI vs ëœë¤ ì„±ëŠ¥ ë¹„êµ ({num_trials}íšŒ í‰ê· )")
        print("="*50)

        # 1. AI ì„±ëŠ¥ ì¸¡ì •
        ai_rewards, ai_throughputs = [], []
        for _ in range(num_trials):
            obs, _ = self.env.reset()
            ai_action, _ = self.model.predict(obs, deterministic=True)
            _, reward, _, _, info = self.env.step(ai_action)
            ai_rewards.append(reward)
            ai_throughputs.append(info['throughput'])
        
        avg_ai_reward = np.mean(ai_rewards)
        avg_ai_throughput = np.mean(ai_throughputs)

        # 2. ëœë¤ ì„±ëŠ¥ ì¸¡ì •
        random_rewards, random_throughputs = [], []
        for _ in range(num_trials):
            obs, _ = self.env.reset()
            random_action = self.env.action_space.sample()
            _, reward, _, _, info = self.env.step(random_action)
            random_rewards.append(reward)
            random_throughputs.append(info['throughput'])

        avg_random_reward = np.mean(random_rewards)
        avg_random_throughput = np.mean(random_throughputs)

        # ê²°ê³¼ ì¶œë ¥
        obs, _ = self.env.reset()
        ai_action, _ = self.model.predict(obs, deterministic=True)
        machines = ai_action + 1

        print(f"ğŸ¤– AI ì—ì´ì „íŠ¸ (í‰ê· ):")
        print(f"  - ëŒ€í‘œ ì „ëµ: ê°€ê³µ({machines[0]}), ì¡°ë¦½({machines[1]}), ê²€ì‚¬({machines[2]})")
        print(f"  - í‰ê·  ì²˜ë¦¬ëŸ‰: {avg_ai_throughput:.1f} ê°œ/ì‹œê°„")
        print(f"  - í‰ê·  ë³´ìƒ: {avg_ai_reward:.2f}")
        print()
        print(f"ğŸ² ëœë¤ ì„ íƒ (í‰ê· ):")
        print(f"  - í‰ê·  ì²˜ë¦¬ëŸ‰: {avg_random_throughput:.1f} ê°œ/ì‹œê°„")
        print(f"  - í‰ê·  ë³´ìƒ: {avg_random_reward:.2f}")
        print()

        # ì„±ëŠ¥ í–¥ìƒë„ ê³„ì‚°
        throughput_improvement = ((avg_ai_throughput - avg_random_throughput) / (avg_random_throughput + 1e-6)) * 100
        reward_improvement = ((avg_ai_reward - avg_random_reward) / (abs(avg_random_reward) + 1e-6)) * 100

        print(f"ğŸ“ˆ AI ì„±ëŠ¥ í–¥ìƒ:")
        print(f"  ì²˜ë¦¬ëŸ‰ í–¥ìƒ: {throughput_improvement:+.1f}%")
        print(f"  ë³´ìƒ í–¥ìƒ: {reward_improvement:+.1f}%")

        if avg_ai_reward > avg_random_reward:
            print("ğŸ† AIê°€ ëœë¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤!")
        else:
            print("ğŸ¤” AIê°€ ë” í•™ìŠµì´ í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤.")

    def save_model(self, filename):
        """í•™ìŠµëœ ëª¨ë¸ ì €ì¥"""
        if not self.is_trained and (self.model is None or self.model.num_timesteps == 0):
            print("âŒ ì €ì¥í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
            return False
        self.model.save(filename)
        print(f"ğŸ’¾ ëª¨ë¸ì´ '{filename}.zip'ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True

    def load_model(self, filename="production_agent_trained"):
        """ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°"""
        try:
            self.model = PPO.load(filename, env=self.env)
            self.is_trained = True
            print(f"ğŸ“‚ ëª¨ë¸ '{filename}.zip'ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
            return True
        except FileNotFoundError:
            print(f"âŒ ëª¨ë¸ íŒŒì¼ '{filename}.zip'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False


# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    print("ğŸš€ ìƒì‚°ë¼ì¸ AI ì—ì´ì „íŠ¸ ì‹¤í–‰")
    print(f"ğŸ”© ì„¤ì •ëœ ìµœëŒ€ ê¸°ê³„ ìˆ˜: {MAX_MACHINES_PER_STATION}ëŒ€/ìŠ¤í…Œì´ì…˜")

    agent = SimpleProductionAgent()

    # í•™ìŠµ ì‹¤í–‰ (ì‹œê°„ì„ ëŠ˜ë ¤ ë” ë‚˜ì€ ì„±ëŠ¥ ê¸°ëŒ€)
    print("\n1ï¸âƒ£ AI í•™ìŠµ ì‹œì‘...")
    # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ë¥¼ í†µí•´ ëª¨ë¸ ì €ì¥ ë° ê·¸ë˜í”„ ì¶œë ¥ë  ìˆ˜ ìˆë„ë¡ total_timesteps ì¡°ì •
    success = agent.train(total_timesteps=200000)

    if success:
        # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        print("\n2ï¸âƒ£ í•™ìŠµëœ AI í…ŒìŠ¤íŠ¸...")
        agent.test_agent(num_tests=5)

        # ëœë¤ê³¼ ë¹„êµ
        print("\n3ï¸âƒ£ ì„±ëŠ¥ ë¹„êµ...")
        agent.compare_with_random(num_trials=10)

        # ëª¨ë¸ ì €ì¥
        print("\n4ï¸âƒ£ ëª¨ë¸ ì €ì¥...")
        agent.save_model(agent.trained_model_filename)

        # í•™ìŠµ ê³¡ì„  ì¶œë ¥
        # í•™ìŠµ ê³¡ì„  ì¶œë ¥ ë° ì €ì¥
        print("\n5ï¸âƒ£ í•™ìŠµ ê³¡ì„  ì¶œë ¥ ë° ì €ì¥...")
        agent.plot_learning_curves(agent.tensorboard_log_dir, save_filename="production_ai_learning_curves.png") # íŒŒì¼ëª… ì§€ì •