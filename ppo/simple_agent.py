from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env
import numpy as np
# ìˆ˜ì •ëœ ë™ì  í™˜ê²½ì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from rl_environment import ProductionLineEnv
# configì—ì„œ ìµœëŒ€ ê¸°ê³„ ëŒ€ìˆ˜ ì„¤ì •ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from config import MAX_MACHINES_PER_STATION

class SimpleProductionAgent:
    """
    ë™ì  ìƒì‚°ë¼ì¸ í™˜ê²½ì— ë§ì¶° í•™ìŠµí•˜ëŠ” AI ì—ì´ì „íŠ¸.
    rl_environment.pyì˜ ë³€ê²½ì‚¬í•­(MultiDiscrete Action Space)ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ì‘ë™í•©ë‹ˆë‹¤.
    """

    def __init__(self):
        self.env = ProductionLineEnv()
        self.model = None
        self.is_trained = False

    def train(self, total_timesteps):
        """AI ì—ì´ì „íŠ¸ í•™ìŠµ"""
        print("ğŸ” í™˜ê²½ ê²€ì¦ ì¤‘...")

        try:
            # check_envê°€ ë™ì ìœ¼ë¡œ ì„¤ì •ëœ í™˜ê²½(MultiDiscrete í¬í•¨)ì„ ê²€ì¦í•©ë‹ˆë‹¤.
            check_env(self.env)
            print("âœ… í™˜ê²½ ê²€ì¦ ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ í™˜ê²½ ì˜¤ë¥˜: {e}")
            return False

        print(f"ğŸ§  AI í•™ìŠµ ì‹œì‘ (ì´ {total_timesteps} ìŠ¤í…)")

        # PPO ì•Œê³ ë¦¬ì¦˜ì€ 'MlpPolicy'ë¥¼ í†µí•´ MultiDiscrete í–‰ë™ ê³µê°„ì„ ì§€ì›í•©ë‹ˆë‹¤.
        self.model = PPO(
            "MlpPolicy",
            self.env,
            verbose=1,
            learning_rate=0.0003,
            n_steps=1024,      # ë” ë§ì€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ í›„ ì—…ë°ì´íŠ¸í•˜ì—¬ ì•ˆì •ì„± í–¥ìƒ
            batch_size=64,
            gamma=0.99,        # ë¯¸ë˜ ë³´ìƒì— ëŒ€í•œ í• ì¸ìœ¨
            ent_coef=0.01,     # íƒí—˜ì„ ì¥ë ¤í•˜ê¸° ìœ„í•œ ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜
            clip_range=0.2,    # PPOì˜ í´ë¦¬í•‘ ë²”ìœ„
            tensorboard_log="./ppo_production_tensorboard/" # í•™ìŠµ ê³¼ì •ì„ í…ì„œë³´ë“œì— ê¸°ë¡
        )

        # í•™ìŠµ ì‹¤í–‰
        self.model.learn(total_timesteps=total_timesteps)
        self.is_trained = True

        print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        return True

    def test_agent(self, num_tests=5):
        """í•™ìŠµëœ ì—ì´ì „íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        if not self.is_trained:
            print("âŒ ë¨¼ì € í•™ìŠµì„ ì™„ë£Œí•´ì£¼ì„¸ìš”!")
            return None

        print(f"\nğŸ§ª í•™ìŠµëœ AI ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ({num_tests}íšŒ)")
        print("="*50)

        results = []
        total_rewards = []
        total_throughputs = []
        total_costs = []

        for test_num in range(num_tests):
            obs, _ = self.env.reset()
            action, _ = self.model.predict(obs, deterministic=True)
            # action ê°’ì— 1ì„ ë”í•´ ì‹¤ì œ ê¸°ê³„ ëŒ€ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤ (0-based -> 1-based)
            machines = action + 1

            next_obs, reward, terminated, truncated, info = self.env.step(action)

            results.append({
                'test_num': test_num + 1,
                'action': machines, # ì‹¤ì œ ê¸°ê³„ ëŒ€ìˆ˜ë¡œ ì €ì¥
                'reward': reward,
                'throughput': info['throughput'],
                'cost': info['total_cost']
            })
            total_rewards.append(reward)
            total_throughputs.append(info['throughput'])
            total_costs.append(info['total_cost'])

            print(f"í…ŒìŠ¤íŠ¸ {test_num + 1}:")
            print(f"  ê¸°ê³„ ë°°ì¹˜: ê°€ê³µ({machines[0]}), ì¡°ë¦½({machines[1]}), ê²€ì‚¬({machines[2]})")
            print(f"  ì²˜ë¦¬ëŸ‰: {info['throughput']:.1f}ê°œ/ì‹œê°„, ë¹„ìš©: ${info['total_cost']}, ë³´ìƒ: {reward:.2f}")

        print("\nğŸ“Š AI ì„±ëŠ¥ ìš”ì•½:")
        print(f"  í‰ê·  ì²˜ë¦¬ëŸ‰: {np.mean(total_throughputs):.1f} ê°œ/ì‹œê°„")
        print(f"  í‰ê·  ë¹„ìš©: ${np.mean(total_costs):.0f}")
        print(f"  í‰ê·  ë³´ìƒ: {np.mean(total_rewards):.2f}")

        return results

    def compare_with_random(self, num_trials=5):
        """ëœë¤ ì„ íƒê³¼ AI ì„±ëŠ¥ ë¹„êµ"""
        if not self.is_trained:
            print("âŒ ë¨¼ì € í•™ìŠµì„ ì™„ë£Œí•´ì£¼ì„¸ìš”!")
            return

        print(f"\nğŸ¥Š AI vs ëœë¤ ì„±ëŠ¥ ë¹„êµ ({num_trials}íšŒ í‰ê· )")
        print("="*50)

        # 1. AI ì„±ëŠ¥ ì¸¡ì •
        ai_rewards, ai_throughputs = [], []
        for _ in range(num_trials):
            obs, _ = self.env.reset()
            ai_action, _ = self.model.predict(obs, deterministic=True)
            _, reward, _, _, info = self.env.step(ai_action)
            ai_rewards.append(reward)
            ai_throughputs.append(info['throughput'])
        
        avg_ai_reward = np.mean(ai_rewards)
        avg_ai_throughput = np.mean(ai_throughputs)

        # 2. ëœë¤ ì„±ëŠ¥ ì¸¡ì •
        random_rewards, random_throughputs = [], []
        for _ in range(num_trials):
            obs, _ = self.env.reset()
            random_action = self.env.action_space.sample()
            _, reward, _, _, info = self.env.step(random_action)
            random_rewards.append(reward)
            random_throughputs.append(info['throughput'])

        avg_random_reward = np.mean(random_rewards)
        avg_random_throughput = np.mean(random_throughputs)

        # ê²°ê³¼ ì¶œë ¥
        obs, _ = self.env.reset()
        ai_action, _ = self.model.predict(obs, deterministic=True)
        machines = ai_action + 1

        print(f"ğŸ¤– AI ì—ì´ì „íŠ¸ (í‰ê· ):")
        print(f"  - ëŒ€í‘œ ì „ëµ: ê°€ê³µ({machines[0]}), ì¡°ë¦½({machines[1]}), ê²€ì‚¬({machines[2]})")
        print(f"  - í‰ê·  ì²˜ë¦¬ëŸ‰: {avg_ai_throughput:.1f} ê°œ/ì‹œê°„")
        print(f"  - í‰ê·  ë³´ìƒ: {avg_ai_reward:.2f}")
        print()
        print(f"ğŸ² ëœë¤ ì„ íƒ (í‰ê· ):")
        print(f"  - í‰ê·  ì²˜ë¦¬ëŸ‰: {avg_random_throughput:.1f} ê°œ/ì‹œê°„")
        print(f"  - í‰ê·  ë³´ìƒ: {avg_random_reward:.2f}")
        print()

        # ì„±ëŠ¥ í–¥ìƒë„ ê³„ì‚°
        throughput_improvement = ((avg_ai_throughput - avg_random_throughput) / (avg_random_throughput + 1e-6)) * 100
        reward_improvement = ((avg_ai_reward - avg_random_reward) / (abs(avg_random_reward) + 1e-6)) * 100

        print(f"ğŸ“ˆ AI ì„±ëŠ¥ í–¥ìƒ:")
        print(f"  ì²˜ë¦¬ëŸ‰ í–¥ìƒ: {throughput_improvement:+.1f}%")
        print(f"  ë³´ìƒ í–¥ìƒ: {reward_improvement:+.1f}%")

        if avg_ai_reward > avg_random_reward:
            print("ğŸ† AIê°€ ëœë¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤!")
        else:
            print("ğŸ¤” AIê°€ ë” í•™ìŠµì´ í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤.")

    def save_model(self, filename="production_agent"):
        """í•™ìŠµëœ ëª¨ë¸ ì €ì¥"""
        if not self.is_trained:
            print("âŒ ì €ì¥í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
            return False
        self.model.save(filename)
        print(f"ğŸ’¾ ëª¨ë¸ì´ '{filename}.zip'ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True

    def load_model(self, filename="production_agent"):
        """ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°"""
        try:
            self.model = PPO.load(filename, env=self.env)
            self.is_trained = True
            print(f"ğŸ“‚ ëª¨ë¸ '{filename}.zip'ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
            return True
        except FileNotFoundError:
            print(f"âŒ ëª¨ë¸ íŒŒì¼ '{filename}.zip'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False


# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    print("ğŸš€ ìƒì‚°ë¼ì¸ AI ì—ì´ì „íŠ¸ ì‹¤í–‰")
    print(f"ğŸ”© ì„¤ì •ëœ ìµœëŒ€ ê¸°ê³„ ìˆ˜: {MAX_MACHINES_PER_STATION}ëŒ€/ìŠ¤í…Œì´ì…˜")

    agent = SimpleProductionAgent()

    # í•™ìŠµ ì‹¤í–‰ (ì‹œê°„ì„ ëŠ˜ë ¤ ë” ë‚˜ì€ ì„±ëŠ¥ ê¸°ëŒ€)
    print("\n1ï¸âƒ£ AI í•™ìŠµ ì‹œì‘...")
    success = agent.train(total_timesteps=100000)

    if success:
        # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        print("\n2ï¸âƒ£ í•™ìŠµëœ AI í…ŒìŠ¤íŠ¸...")
        agent.test_agent(num_tests=5)

        # ëœë¤ê³¼ ë¹„êµ
        print("\n3ï¸âƒ£ ì„±ëŠ¥ ë¹„êµ...")
        agent.compare_with_random(num_trials=10)

        # ëª¨ë¸ ì €ì¥
        print("\n4ï¸âƒ£ ëª¨ë¸ ì €ì¥...")
        agent.save_model("my_production_ai")